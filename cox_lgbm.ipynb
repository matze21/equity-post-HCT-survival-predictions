{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = pd.read_csv('data_dictionary.csv')\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTransformer:\n",
    "    def __init__(self):\n",
    "        self.dummy_features = [\n",
    "            'dri_score', 'psych_disturb', 'diabetes', 'tbi_status', 'arrhythmia',\n",
    "            'graft_type', 'vent_hist', 'renal_issue', 'pulm_severe', 'prim_disease_hct',\n",
    "            'cmv_status', 'tce_imm_match', 'rituximab', 'prod_type', 'ethnicity',\n",
    "            'obesity', 'mrd_hct', 'in_vivo_tcd', 'tce_match', 'hepatic_severe',\n",
    "            'prior_tumor', 'peptic_ulcer', 'gvhd_proph', 'rheum_issue', 'sex_match',\n",
    "            'race_group', 'hepatic_mild', 'tce_div_match', 'donor_related', 'melphalan_dose',\n",
    "            'cardiac', 'pulm_moderate'\n",
    "        ]\n",
    "        \n",
    "        self.numeric_features = [\n",
    "            'cyto_score', 'hla_match_c_high', 'hla_high_res_8', 'hla_low_res_6',\n",
    "            'hla_high_res_6', 'hla_high_res_10', 'hla_match_dqb1_high', 'hla_nmdp_6',\n",
    "            'hla_match_c_low', 'hla_match_drb1_low', 'hla_match_dqb1_low',\n",
    "            'cyto_score_detail', 'conditioning_intensity', 'year_hct', 'hla_match_a_high',\n",
    "            'hla_match_b_low', 'hla_match_a_low', 'hla_match_b_high', 'comorbidity_score',\n",
    "            'karnofsky_score', 'hla_low_res_8', 'hla_match_drb1_high', 'hla_low_res_10',\n",
    "            'donor_age', 'age_at_hct', 'efs_time'\n",
    "        ]\n",
    "        \n",
    "        self.encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        self.imputer = SimpleImputer(strategy='constant', fill_value='Missing')\n",
    "        self.numeric_imputer = SimpleImputer(strategy='median')\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "        self.cyto_score_rank = {'Favorable': 0, 'Intermediate': 1, 'Poor': 2, 'Other': 1.5, 'TBD': np.nan, 'Not tested': np.nan}\n",
    "        self.cyto_score_detail_rank = {'Favorable': 0, 'Intermediate': 1, 'Poor': 2, 'TBD': np.nan, 'Not tested': np.nan}\n",
    "        self.conditioning_intensity_rank = {'NMA': 0, 'RIC': 1, 'MAC': 2, 'TBD': np.nan, 'No drugs reported': np.nan, 'N/A, F(pre-TED) not submitted': np.nan}\n",
    "\n",
    "        self.earliest_year = None   \n",
    "    \n",
    "    def encode_race_group(self, df):\n",
    "        race_groups = df['race_group'].unique()\n",
    "        self.race_group_mapping = {race: i for i, race in enumerate(race_groups)}\n",
    "\n",
    "\n",
    "    def fit(self, df):\n",
    "        df = df.copy()\n",
    "\n",
    "        self.encode_race_group(df)\n",
    "        \n",
    "        # Apply custom ranking for specific features\n",
    "        df['cyto_score'] = df['cyto_score'].map(self.cyto_score_rank)\n",
    "        df['cyto_score_detail'] = df['cyto_score_detail'].map(self.cyto_score_detail_rank)\n",
    "        df['conditioning_intensity'] = df['conditioning_intensity'].map(self.conditioning_intensity_rank)\n",
    "        \n",
    "        # Add age difference feature\n",
    "        df['age_difference'] = df['donor_age'] - df['age_at_hct']\n",
    "        \n",
    "        # Handle year_hct\n",
    "        self.earliest_year = df['year_hct'].min()\n",
    "        df['years_since_first_hct'] = df['year_hct'] - self.earliest_year\n",
    "        \n",
    "        # Update numeric_features list\n",
    "        self.numeric_features = [f for f in self.numeric_features if f != 'year_hct'] + ['age_difference', 'years_since_first_hct']\n",
    "        \n",
    "        # Now proceed with the rest of the fitting process\n",
    "        imputed_data = self.imputer.fit_transform(df[self.dummy_features])\n",
    "        self.numeric_imputer.fit(df[self.numeric_features])\n",
    "        self.scaler.fit(df[self.numeric_features])\n",
    "        \n",
    "        # Fit the encoder on the imputed data\n",
    "        self.encoder.fit(imputed_data)\n",
    "    \n",
    "\n",
    "    def transform(self, df):\n",
    "        df_transformed = df.copy()\n",
    "        \n",
    "        # Apply custom ranking for specific features\n",
    "        df_transformed['cyto_score'] = df_transformed['cyto_score'].map(self.cyto_score_rank)\n",
    "        df_transformed['cyto_score_detail'] = df_transformed['cyto_score_detail'].map(self.cyto_score_detail_rank)\n",
    "        df_transformed['conditioning_intensity'] = df_transformed['conditioning_intensity'].map(self.conditioning_intensity_rank)\n",
    "        \n",
    "        # Add age difference feature\n",
    "        df_transformed['age_difference'] = df_transformed['donor_age'] - df_transformed['age_at_hct']\n",
    "        \n",
    "        # Handle year_hct\n",
    "        df_transformed['years_since_first_hct'] = df_transformed['year_hct'] - self.earliest_year\n",
    "        df_transformed = df_transformed.drop(columns=['year_hct'])\n",
    "        \n",
    "        # Handle numeric features\n",
    "        for feature in self.numeric_features:\n",
    "            if feature in df_transformed.columns:\n",
    "                df_transformed[feature] = pd.to_numeric(df_transformed[feature], errors='coerce')\n",
    "        \n",
    "        # Impute missing values in numeric features\n",
    "        df_transformed[self.numeric_features] = self.numeric_imputer.transform(df_transformed[self.numeric_features])\n",
    "        \n",
    "        # Normalize numeric values\n",
    "        df_transformed[self.numeric_features] = self.scaler.transform(df_transformed[self.numeric_features])\n",
    "        \n",
    "        # Handle categorical features\n",
    "        imputed_data = self.imputer.transform(df_transformed[self.dummy_features])\n",
    "        dummy_encoded = self.encoder.transform(imputed_data)\n",
    "        dummy_columns = self.encoder.get_feature_names_out(self.dummy_features)\n",
    "        dummy_df = pd.DataFrame(dummy_encoded, columns=dummy_columns, index=df_transformed.index)\n",
    "        \n",
    "        # Drop original dummy features and concatenate encoded features\n",
    "        df_transformed = df_transformed.drop(columns=self.dummy_features)\n",
    "        df_transformed = pd.concat([df_transformed, dummy_df], axis=1)\n",
    "        \n",
    "        # Handle 'efs' separately as it's binary\n",
    "        if 'efs' in df_transformed.columns:\n",
    "            df_transformed['efs'] = df_transformed['efs'].astype(int)\n",
    "\n",
    "        df_transformed['race_group'] = df['race_group'].map(self.race_group_mapping)\n",
    "        \n",
    "        return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_split, test_df_split = train_test_split(train, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "transformer = FeatureTransformer()\n",
    "transformer.fit(train_df_split)\n",
    "train_df_split = transformer.transform(train_df_split)\n",
    "test_df_split = transformer.transform(test_df_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines.utils import concordance_index\n",
    "\n",
    "\"\"\"\n",
    "To evaluate the equitable prediction of transplant survival outcomes,\n",
    "we use the concordance index (C-index) between a series of event\n",
    "times and a predicted score across each race group.\n",
    " \n",
    "It represents the global assessment of the model discrimination power:\n",
    "this is the modelâ€™s ability to correctly provide a reliable ranking\n",
    "of the survival times based on the individual risk scores.\n",
    " \n",
    "The concordance index is a value between 0 and 1 where:\n",
    " \n",
    "0.5 is the expected result from random predictions,\n",
    "1.0 is perfect concordance (with no censoring, otherwise <1.0),\n",
    "0.0 is perfect anti-concordance (with no censoring, otherwise >0.0)\n",
    "\n",
    "\"\"\"\n",
    "import pandas.api.types\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name : str) -> float:\n",
    "    \"\"\"\n",
    "    >>> import pandas as pd\n",
    "    >>> row_id_column_name = \"id\"\n",
    "    >>> y_pred = {'prediction': {0: 1.0, 1: 0.0, 2: 1.0}}\n",
    "    >>> y_pred = pd.DataFrame(y_pred)\n",
    "    >>> y_pred.insert(0, row_id_column_name, range(len(y_pred)))\n",
    "    >>> y_true = { 'efs': {0: 1.0, 1: 0.0, 2: 0.0}, 'efs_time': {0: 25.1234,1: 250.1234,2: 2500.1234}, 'race_group': {0: 'race_group_1', 1: 'race_group_1', 2: 'race_group_1'}}\n",
    "    >>> y_true = pd.DataFrame(y_true)\n",
    "    >>> y_true.insert(0, row_id_column_name, range(len(y_true)))\n",
    "    >>> score(y_true.copy(), y_pred.copy(), row_id_column_name)\n",
    "    0.75\n",
    "    \"\"\"\n",
    "    \n",
    "    #del solution[row_id_column_name]\n",
    "    del submission[row_id_column_name]\n",
    "    \n",
    "    event_label = 'efs'\n",
    "    interval_label = 'efs_time'\n",
    "    prediction_label = 'prediction'\n",
    "    # Merging solution and submission dfs on ID\n",
    "    merged_df = pd.concat([solution, submission], axis=1)\n",
    "    merged_df.reset_index(inplace=True)\n",
    "    merged_df_race_dict = dict(merged_df.groupby(['race_group']).groups)\n",
    "    metric_list = []\n",
    "    for race in merged_df_race_dict.keys():\n",
    "        # Retrieving values from y_test based on index\n",
    "        indices = sorted(merged_df_race_dict[race])\n",
    "        merged_df_race = merged_df.iloc[indices]\n",
    "        # Calculate the concordance index\n",
    "        c_index_race = concordance_index(\n",
    "                        merged_df_race[interval_label],\n",
    "                        -merged_df_race[prediction_label],\n",
    "                        merged_df_race[event_label])\n",
    "        metric_list.append(c_index_race)\n",
    "    return float(np.mean(metric_list)-np.sqrt(np.var(metric_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xgboost cox model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "train_df_split.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in train_df_split.columns.values]\n",
    "test_df_split.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in test_df_split.columns.values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "X_train = train_df_split.drop(['ID', 'efs', 'efs_time'], axis=1)\n",
    "y_train_time = train_df_split['efs_time']\n",
    "y_train_event = train_df_split['efs']\n",
    "\n",
    "X_test = test_df_split.drop(['ID', 'efs', 'efs_time'], axis=1)\n",
    "y_test_time = test_df_split['efs_time']\n",
    "y_test_event = test_df_split['efs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Create XGBoost DMatrix objects\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train_time, weight=y_train_event)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test_time, weight=y_test_event)\n",
    "\n",
    "# Set parameters for XGBoost\n",
    "params = {\n",
    "    \"objective\": \"survival:cox\",\n",
    "    \"eval_metric\": \"cox-nloglik\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    #\"max_depth\": 3,\n",
    "    #\"learning_rate\": 0.05,\n",
    "    #\"subsample\": 0.8,\n",
    "    #\"colsample_bytree\": 0.8\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=100,\n",
    "    evals=[(dtrain, 'train'), (dtest, 'test')],\n",
    "    #feval=xgb_score,\n",
    "    verbose_eval=10  # Log every 10 iterations\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = pd.DataFrame({'ID':train_df_split['ID'],'prediction':model.predict(dtrain)})\n",
    "y_pred_test = pd.DataFrame({'ID':test_df_split['ID'],'prediction':model.predict(dtest)})\n",
    "\n",
    "# Calculate final scores\n",
    "train_score = score(train_df_split, y_pred_train, 'ID')\n",
    "test_score = score(test_df_split, y_pred_test, 'ID')\n",
    "\n",
    "print(f\"Final Train Score: {train_score}\")\n",
    "print(f\"Final Test Score: {test_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lgbm cox model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Prepare the data\n",
    "X_train = train_df_split.drop(['ID','efs', 'efs_time'], axis=1)\n",
    "y_train = train_df_split[['efs', 'efs_time']]\n",
    "\n",
    "X_test = test_df_split.drop(['ID','efs', 'efs_time'], axis=1)\n",
    "y_test = test_df_split[['efs', 'efs_time']]\n",
    "\n",
    "# Create LightGBM datasets\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "# Set parameters for LightGBM\n",
    "params = {\n",
    "    \"objective\": \"survival:cox\",  # Cox model\n",
    "    \"metric\": \"cox\",  # Concordance index\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Custom evaluation function\n",
    "def lgb_score(preds, train_data):\n",
    "    y_true = train_data.get_label()\n",
    "    score = calculate_score(train_data.data, preds)\n",
    "    return 'custom_score', score, True\n",
    "\n",
    "# Train the model\n",
    "model = lgb.train(params,\n",
    "                  lgb_train,\n",
    "                  num_boost_round=100,\n",
    "                  valid_sets=[lgb_train, lgb_eval],\n",
    "                  feval=lgb_score,\n",
    "                  callbacks=[lgb.log_evaluation(10)])  # Log every 10 iterations\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Calculate final scores\n",
    "train_score = calculate_score(train_df_split, y_pred_train)\n",
    "test_score = calculate_score(test_df_split, y_pred_test)\n",
    "\n",
    "print(f\"Final Train Score: {train_score}\")\n",
    "print(f\"Final Test Score: {test_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
