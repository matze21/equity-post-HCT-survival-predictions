{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = pd.read_csv('data_dictionary.csv')\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class FeatureTransformer:\n",
    "    def __init__(self):\n",
    "        self.categorical_features = [\n",
    "            'dri_score', 'psych_disturb', 'diabetes', 'tbi_status', 'arrhythmia',\n",
    "            'graft_type', 'vent_hist', 'renal_issue', 'pulm_severe', 'prim_disease_hct',\n",
    "            'cmv_status', 'tce_imm_match', 'rituximab', 'prod_type', 'ethnicity',\n",
    "            'obesity', 'mrd_hct', 'in_vivo_tcd', 'tce_match', 'hepatic_severe',\n",
    "            'prior_tumor', 'peptic_ulcer', 'gvhd_proph', 'rheum_issue', 'sex_match',\n",
    "            'race_group', 'hepatic_mild', 'tce_div_match', 'donor_related', 'melphalan_dose',\n",
    "            'cardiac', 'pulm_moderate'\n",
    "        ]\n",
    "        \n",
    "        self.numeric_features = [\n",
    "            'hla_match_c_high', 'hla_high_res_8', 'hla_low_res_6',\n",
    "            'hla_high_res_6', 'hla_high_res_10', 'hla_match_dqb1_high', 'hla_nmdp_6',\n",
    "            'hla_match_c_low', 'hla_match_drb1_low', 'hla_match_dqb1_low',\n",
    "            'comorbidity_score', 'karnofsky_score', 'hla_low_res_8', 'hla_match_drb1_high', \n",
    "            'hla_low_res_10', 'donor_age', 'age_at_hct'\n",
    "        ]\n",
    "        \n",
    "        self.ordinal_features = ['cyto_score', 'cyto_score_detail', 'conditioning_intensity']\n",
    "        \n",
    "        self.imputer = SimpleImputer(strategy='constant', fill_value='Missing')\n",
    "        self.numeric_imputer = SimpleImputer(strategy='median')\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "        self.cyto_score_rank = {'Favorable': 0, 'Intermediate': 1, 'Poor': 2, 'Other': 1.5, 'TBD': np.nan, 'Not tested': np.nan}\n",
    "        self.cyto_score_detail_rank = {'Favorable': 0, 'Intermediate': 1, 'Poor': 2, 'TBD': np.nan, 'Not tested': np.nan}\n",
    "        self.conditioning_intensity_rank = {'NMA': 0, 'RIC': 1, 'MAC': 2, 'TBD': np.nan, 'No drugs reported': np.nan, 'N/A, F(pre-TED) not submitted': np.nan}\n",
    "\n",
    "        self.earliest_year = None\n",
    "\n",
    "    def fit(self, df):\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Handle ordinal features\n",
    "        for feature in self.ordinal_features:\n",
    "            df[feature] = df[feature].map(getattr(self, f\"{feature}_rank\"))\n",
    "        \n",
    "        # Add age difference feature\n",
    "        df['age_difference'] = df['donor_age'] - df['age_at_hct']\n",
    "        \n",
    "        # Handle year_hct\n",
    "        self.earliest_year = df['year_hct'].min()\n",
    "        df['years_since_first_hct'] = df['year_hct'] - self.earliest_year\n",
    "        \n",
    "        # Update numeric_features list\n",
    "        self.numeric_features += ['age_difference', 'years_since_first_hct']\n",
    "        \n",
    "        # Fit imputers and scaler\n",
    "        self.imputer.fit(df[self.categorical_features])\n",
    "        self.numeric_imputer.fit(df[self.numeric_features])\n",
    "        self.scaler.fit(df[self.numeric_features])\n",
    "\n",
    "    def transform(self, df):\n",
    "        df_transformed = df.copy()\n",
    "        \n",
    "        # Handle ordinal features\n",
    "        for feature in self.ordinal_features:\n",
    "            df_transformed[feature] = df_transformed[feature].map(getattr(self, f\"{feature}_rank\"))\n",
    "        \n",
    "        # Add age difference feature\n",
    "        df_transformed['age_difference'] = df_transformed['donor_age'] - df_transformed['age_at_hct']\n",
    "        \n",
    "        # Handle year_hct\n",
    "        df_transformed['years_since_first_hct'] = df_transformed['year_hct'] - self.earliest_year\n",
    "        df_transformed = df_transformed.drop(columns=['year_hct'])\n",
    "        \n",
    "        # Impute missing values\n",
    "        df_transformed[self.categorical_features] = self.imputer.transform(df_transformed[self.categorical_features])\n",
    "        df_transformed[self.numeric_features] = self.numeric_imputer.transform(df_transformed[self.numeric_features])\n",
    "        \n",
    "        # Normalize numeric values\n",
    "        df_transformed[self.numeric_features] = self.scaler.transform(df_transformed[self.numeric_features])\n",
    "        \n",
    "        # Convert categorical features to 'category' dtype\n",
    "        for feature in self.categorical_features:\n",
    "            df_transformed[feature] = df_transformed[feature].astype('category')\n",
    "        \n",
    "        # Handle 'efs' separately as it's binary\n",
    "        if 'efs' in df_transformed.columns:\n",
    "            df_transformed['efs'] = df_transformed['efs'].astype(int)\n",
    "            df_transformed[\"efs_time2\"] = df_transformed.efs_time.copy()\n",
    "            df_transformed.loc[df_transformed.efs==0,\"efs_time2\"] *= -1\n",
    "        \n",
    "        return df_transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_split, test_df_split = train_test_split(train, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "transformer = FeatureTransformer()\n",
    "transformer.fit(train_df_split)\n",
    "train_df_split = transformer.transform(train_df_split)\n",
    "test_df_split = transformer.transform(test_df_split)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xgboost cox model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "train_df_split.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in train_df_split.columns.values]\n",
    "test_df_split.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in test_df_split.columns.values]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = train_df_split.drop(['ID', 'efs', 'efs_time','efs_time2'], axis=1).columns\n",
    "train_df_split.reset_index(inplace=True)\n",
    "test_df_split.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "FOLDS = 5\n",
    "kf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "    \n",
    "oof_xgb = np.zeros(len(train_df_split))\n",
    "pred_xgb = np.zeros(len(test_df_split))\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(train_df_split)):\n",
    "\n",
    "    print(\"#\"*25)\n",
    "    print(f\"### Fold {i+1}\")\n",
    "    print(\"#\"*25)\n",
    "    \n",
    "    x_train = train_df_split.loc[train_index,FEATURES].copy()\n",
    "    y_train = train_df_split.loc[train_index,\"efs_time2\"]\n",
    "    x_valid = train_df_split.loc[test_index,FEATURES].copy()\n",
    "    y_valid = train_df_split.loc[test_index,\"efs_time2\"]\n",
    "    x_test = test_df_split[FEATURES].copy()\n",
    "\n",
    "    model_xgb = XGBRegressor(\n",
    "        max_depth=3,  \n",
    "        colsample_bytree=0.5, \n",
    "        subsample=0.8, \n",
    "        n_estimators=10_000,  \n",
    "        learning_rate=0.1, \n",
    "        #eval_metric=\"mae\",\n",
    "        early_stopping_rounds=25,\n",
    "        #objective='reg:logistic',\n",
    "        enable_categorical=True,\n",
    "        min_child_weight=5,\n",
    "        objective = \"survival:cox\",\n",
    "        eval_metric= \"cox-nloglik\",\n",
    "    )\n",
    "    model_xgb.fit(\n",
    "        x_train, y_train,\n",
    "        eval_set=[(x_valid, y_valid)],  \n",
    "        verbose=100 \n",
    "    )\n",
    "\n",
    "    # INFER OOF\n",
    "    oof_xgb[test_index] = model_xgb.predict(x_valid)\n",
    "    # INFER TEST\n",
    "    pred_xgb += model_xgb.predict(x_test)\n",
    "\n",
    "# COMPUTE AVERAGE TEST PREDS\n",
    "pred_xgb /= FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metric import score\n",
    "\n",
    "y_true = train_df_split[[\"ID\",\"efs\",\"efs_time\",\"race_group\"]].copy()\n",
    "y_pred = train_df_split[[\"ID\"]].copy()\n",
    "y_pred[\"prediction\"] = oof_xgb #higher risk should lead to lower value, so our prediction is just simply a risk score\n",
    "m = score(y_true.copy(), y_pred.copy(), \"ID\")\n",
    "\n",
    "y_true = test_df_split[[\"ID\",\"efs\",\"efs_time\",\"race_group\"]].copy()\n",
    "y_pred = test_df_split[[\"ID\"]].copy()\n",
    "y_pred[\"prediction\"] = pred_xgb #higher risk should lead to lower value, so our prediction is just simply a risk score\n",
    "n = score(y_true.copy(), y_pred.copy(), \"ID\")\n",
    "#print(f\"\\nOverall CV for XGBoost =\",m)\n",
    "#print(f\"\\nOverall test for XGBoost =\",n)\n",
    "print(f\"CV: {m} | Test: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "basic model from kaggle: 0.6677549507917714\n",
    "\n",
    "full feature one hot encoding, imputing, normalizing, target normalizing\n",
    "CV: 0.435125700420623 | Test: 0.43775461409778854\n",
    "\n",
    "full feature one hot encoding, imputing, normalizing\n",
    "CV: 0.6568945009770654 | Test: 0.6611580849857914\n",
    "\n",
    "categorical values, imputing, normalizing\n",
    "CV: 0.6624635547259778 | Test: 0.6641153857333534\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## no cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "X_train = train_df_split.drop(['ID', 'efs', 'efs_time','efs_time2'], axis=1)\n",
    "y_train = train_df_split['efs_time2']\n",
    "\n",
    "X_test = test_df_split.drop(['ID', 'efs', 'efs_time','efs_time2'], axis=1)\n",
    "y_test = test_df_split['efs_time2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from metric import score\n",
    "\n",
    "\n",
    "\n",
    "# Create XGBoost DMatrix objects\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Set parameters for XGBoost\n",
    "params = {\n",
    "    \"objective\": \"survival:cox\",\n",
    "    \"eval_metric\": \"cox-nloglik\",\n",
    "    \"max_depth\":3,  \n",
    "    \"colsample_bytree\":0.5, \n",
    "    \"subsample\":0.8, \n",
    "    \"n_estimators\":10_000,  \n",
    "    \"learning_rate\":0.1, \n",
    "    \"min_child_weight\":5,\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=1000,\n",
    "    evals=[(dtrain, 'train'), (dtest, 'test')],\n",
    "    #feval=xgb_score,\n",
    "    verbose_eval=100  # Log every 10 iterations\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = pd.DataFrame({'ID':train_df_split['ID'],'prediction':model.predict(dtrain)})\n",
    "y_pred_test = pd.DataFrame({'ID':test_df_split['ID'],'prediction':model.predict(dtest)})\n",
    "\n",
    "# Calculate final scores\n",
    "train_score = score(train_df_split, y_pred_train, 'ID')\n",
    "test_score = score(test_df_split, y_pred_test, 'ID')\n",
    "\n",
    "print(f\"Final Train Score: {train_score}\")\n",
    "print(f\"Final Test Score: {test_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metric import score\n",
    "\n",
    "y_true = train[[\"ID\",\"efs\",\"efs_time\",\"race_group\"]].copy()\n",
    "y_pred = train[[\"ID\"]].copy()\n",
    "y_pred[\"prediction\"] = oof_xgb #higher risk should lead to lower value, so our prediction is just simply a risk score\n",
    "m = score(y_true.copy(), y_pred.copy(), \"ID\")\n",
    "print(f\"\\nOverall CV for XGBoost =\",m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
