{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from lifelines import KaplanMeierFitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = pd.read_csv('data_dictionary.csv')\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMV = [\"ID\",\"efs\",\"efs_time\",\"y\", 'efs_time2','log_survival_probability']\n",
    "FEATURES = [c for c in train.columns if not c in RMV]\n",
    "print(f\"There are {len(FEATURES)} FEATURES: {FEATURES}\")\n",
    "\n",
    "CATS = []\n",
    "for c in FEATURES:\n",
    "    if train[c].dtype==\"object\":\n",
    "        CATS.append(c)\n",
    "        train[c] = train[c].fillna(\"NAN\")\n",
    "        test[c] = test[c].fillna(\"NAN\")\n",
    "print(f\"In these features, there are {len(CATS)} CATEGORICAL FEATURES: {CATS}\")\n",
    "\n",
    "combined = pd.concat([train,test],axis=0,ignore_index=True)\n",
    "#print(\"Combined data shape:\", combined.shape )\n",
    "\n",
    "# LABEL ENCODE CATEGORICAL FEATURES\n",
    "print(\"We LABEL ENCODE the CATEGORICAL FEATURES: \",end=\"\")\n",
    "for c in FEATURES:\n",
    "\n",
    "    # LABEL ENCODE CATEGORICAL AND CONVERT TO INT32 CATEGORY\n",
    "    if c in CATS:\n",
    "        print(f\"{c}, \",end=\"\")\n",
    "        combined[c],_ = combined[c].factorize()\n",
    "        combined[c] -= combined[c].min()\n",
    "        combined[c] = combined[c].astype(\"int32\")\n",
    "        combined[c] = combined[c].astype(\"category\")\n",
    "        \n",
    "    # REDUCE PRECISION OF NUMERICAL TO 32BIT TO SAVE MEMORY\n",
    "    else:\n",
    "        if combined[c].dtype==\"float64\":\n",
    "            combined[c] = combined[c].astype(\"float32\")\n",
    "        if combined[c].dtype==\"int64\":\n",
    "            combined[c] = combined[c].astype(\"int32\")\n",
    "    \n",
    "train_df_split0 = combined.iloc[:len(train)].copy()\n",
    "submit_df_split = combined.iloc[len(train):].reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_split0[\"efs_time2\"] = train_df_split0.efs_time.copy()\n",
    "train_df_split0.loc[train_df_split0.efs==0,\"efs_time2\"] *= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_split, test_df_split = train_test_split(train_df_split0, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmf_models = {}\n",
    "for race in train_df_split['race_group'].unique():\n",
    "    kmf = KaplanMeierFitter()\n",
    "    mask = train_df_split['race_group'] == race\n",
    "    kmf.fit(train_df_split[mask]['efs_time'], train_df_split[mask]['efs'], label=race)\n",
    "    kmf_models[race] = kmf\n",
    "\n",
    "def get_survival_probability(row):\n",
    "    race = row['race_group']\n",
    "    time = row['efs_time']\n",
    "    return kmf_models[race].survival_function_at_times(time).iloc[0] *1/(row.efs_time + 1)\n",
    "\n",
    "train_df_split['survival_probability'] = train_df_split.apply(get_survival_probability, axis=1)\n",
    "test_df_split['survival_probability'] = train_df_split.apply(get_survival_probability, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "qt = QuantileTransformer(n_quantiles=1000, output_distribution='normal', random_state=42)\n",
    "train_df_split['q_survival_probability'] = qt.fit_transform(train_df_split['survival_probability'].values.reshape(-1, 1))\n",
    "train_df_split['q_survival_probability'] = train_df_split['q_survival_probability']\n",
    "\n",
    "test_df_split['q_survival_probability'] = qt.transform(test_df_split['survival_probability'].values.reshape(-1, 1))\n",
    "test_df_split['q_survival_probability'] = test_df_split['q_survival_probability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = train_df_split.drop(['ID', 'efs', 'efs_time','survival_probability','q_survival_probability','efs_time2'], axis=1).columns\n",
    "train_df_split.reset_index(inplace=True)\n",
    "test_df_split.reset_index(inplace=True)\n",
    "submit_df_split.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xgboost kaplan meier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "import xgboost\n",
    "from sklearn.ensemble import StackingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "FOLDS = 5\n",
    "kf = KFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "    \n",
    "oof_xgb = np.zeros(len(train_df_split))\n",
    "pred_xgb = np.zeros(len(test_df_split))\n",
    "submit_xgb = np.zeros(len(submit_df_split))\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(train_df_split)):\n",
    "\n",
    "    print(\"#\"*25)\n",
    "    print(f\"### Fold {i+1}\")\n",
    "    print(\"#\"*25)\n",
    "    \n",
    "    x_train = train_df_split.loc[train_index,FEATURES].copy()\n",
    "    y_train = train_df_split.loc[train_index,\"q_survival_probability\"]\n",
    "    y_train2 = train_df_split.loc[train_index,\"efs_time2\"]\n",
    "    x_valid = train_df_split.loc[test_index,FEATURES].copy()\n",
    "\n",
    "    y_valid = train_df_split.loc[test_index,\"q_survival_probability\"]\n",
    "    y_valid2 = train_df_split.loc[test_index,\"efs_time2\"]\n",
    "    x_test = test_df_split[FEATURES].copy()\n",
    "    x_submit = submit_df_split[FEATURES].copy()\n",
    "\n",
    "    dtrain = xgboost.DMatrix(x_train, label=y_train, enable_categorical=True)\n",
    "    dvalid = xgboost.DMatrix(x_valid, label=y_valid, enable_categorical=True)\n",
    "\n",
    "    model_kaplan = XGBRegressor(\n",
    "        max_depth=5,  \n",
    "        colsample_bytree=0.5, \n",
    "        subsample=0.8, \n",
    "        n_estimators=300,  \n",
    "        learning_rate=0.1, \n",
    "        early_stopping_rounds=25,\n",
    "        #objective='reg:logistic',\n",
    "        enable_categorical=True,\n",
    "        min_child_weight=5,\n",
    "        eval_metric= \"rmse\",\n",
    "    )\n",
    "\n",
    "    model_kaplan.fit(\n",
    "        x_train, y_train,\n",
    "        eval_set=[(x_train, y_train), (x_valid, y_valid)],  \n",
    "        verbose=100 \n",
    "    )\n",
    "\n",
    "    model_cox = XGBRegressor(\n",
    "        max_depth=3,  \n",
    "        colsample_bytree=0.5, \n",
    "        subsample=0.8, \n",
    "        n_estimators=300,  \n",
    "        learning_rate=0.1, \n",
    "        #eval_metric=\"mae\",\n",
    "        early_stopping_rounds=25,\n",
    "        #objective='reg:logistic',\n",
    "        enable_categorical=True,\n",
    "        min_child_weight=5,\n",
    "        objective = \"survival:cox\",\n",
    "        eval_metric= \"cox-nloglik\",\n",
    "    )\n",
    "    \n",
    "\n",
    "    model_cox.fit(\n",
    "        x_train, y_train2,\n",
    "        eval_set=[(x_valid, y_valid2)],  \n",
    "        verbose=100 \n",
    "    )\n",
    "\n",
    "    predCox = model_cox.predict(x_train)\n",
    "    predKaplan = model_kaplan.predict(x_train)\n",
    "    x_train[\"cox\"] = predCox\n",
    "    x_train[\"kaplan\"] = predKaplan\n",
    "\n",
    "    redCoxVal = model_cox.predict(x_valid)\n",
    "    predKaplanVal = model_kaplan.predict(x_valid)\n",
    "    x_valid[\"cox\"] = redCoxVal\n",
    "    x_valid[\"kaplan\"] = predKaplanVal\n",
    "\n",
    "    redCoxTest = model_cox.predict(x_test)\n",
    "    predKaplanTest = model_kaplan.predict(x_test)\n",
    "    x_test[\"cox\"] = redCoxTest\n",
    "    x_test[\"kaplan\"] = predKaplanTest\n",
    "\n",
    "    redCoxSubmit = model_cox.predict(x_submit)\n",
    "    predKaplanSubmit = model_kaplan.predict(x_submit)\n",
    "    x_submit[\"cox\"] = redCoxSubmit\n",
    "    x_submit[\"kaplan\"] = predKaplanSubmit\n",
    "\n",
    "    stacking_model = XGBRegressor(\n",
    "        max_depth=5,  \n",
    "        colsample_bytree=0.5, \n",
    "        subsample=0.8, \n",
    "        n_estimators=300,  \n",
    "        learning_rate=0.1, \n",
    "        early_stopping_rounds=25,\n",
    "        #objective='reg:logistic',\n",
    "        enable_categorical=True,\n",
    "        min_child_weight=5,\n",
    "        eval_metric= \"rmse\")\n",
    "\n",
    "    # define the stacking model\n",
    "    #stacking_model = StackingRegressor(estimators=[('cox', model_cox), ('kaplan', model_kaplan)], final_estimator=XGBRegressor()) #, early_stopping_rounds=5, eval_set=[(x_valid, y_valid)])\n",
    "\n",
    "    # fit the stacking model to the training data\n",
    "    stacking_model.fit(x_train, y_train, eval_set=[(x_train, y_train), (x_valid, y_valid)], verbose=100 )\n",
    "\n",
    "    # INFER OOF\n",
    "    oof_xgb[test_index] = stacking_model.predict(x_valid)\n",
    "    # INFER TEST\n",
    "    pred_xgb += stacking_model.predict(x_test)\n",
    "    # INFER SUBMIT\n",
    "    submit_xgb += stacking_model.predict(x_submit)\n",
    "\n",
    "# COMPUTE AVERAGE TEST PREDS\n",
    "pred_xgb /= FOLDS\n",
    "submit_xgb /= FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_xgbR = qt.inverse_transform(oof_xgb.reshape(-1,1))\n",
    "pred_xgbR = qt.inverse_transform(pred_xgb.reshape(-1,1))\n",
    "submit_xgb = qt.inverse_transform(submit_xgb.reshape(-1,1))\n",
    "\n",
    "from metric import score\n",
    "\n",
    "y_trueCV = train_df_split[[\"ID\",\"efs\",\"efs_time\",\"race_group\",'survival_probability']].copy()\n",
    "y_predCV = train_df_split[[\"ID\"]].copy()\n",
    "y_predCV[\"prediction\"] = oof_xgbR #higher risk should lead to lower value, so our prediction is just simply a risk score\n",
    "m, ar0 = score(y_trueCV.copy(), y_predCV.copy(), \"ID\")\n",
    "\n",
    "\n",
    "y_true = test_df_split[[\"ID\",\"efs\",\"efs_time\",\"race_group\",'survival_probability']].copy()\n",
    "y_pred = test_df_split[[\"ID\"]].copy()\n",
    "y_pred[\"prediction\"] = pred_xgbR #higher risk should lead to lower value, so our prediction is just simply a risk score\n",
    "n, ar1 = score(y_true.copy(), y_pred.copy(), \"ID\")\n",
    "#print(f\"\\nOverall CV for XGBoost =\",m)\n",
    "#print(f\"\\nOverall test for XGBoost =\",n)\n",
    "print(f\"CV: {m} | Test: {n}\")\n",
    "#print(f\"c-indexes: CV: {ar0} | Test: {ar1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_submit = submit_df_split[[\"ID\"]].copy()\n",
    "y_submit[\"prediction\"] = submit_xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
